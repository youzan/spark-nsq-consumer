package org.apache.spark.streaming.nsq

import com.typesafe.scalalogging.slf4j.LazyLogging
import com.youzan.bigdata.streaming.nsq.{BaseMessageHandler, NSQMessageWrapper}
import com.youzan.nsq.client.MessageHandler
import org.apache.spark.storage.{StorageLevel, StreamBlockId}
import org.apache.spark.streaming.receiver.{BlockGenerator, BlockGeneratorListener}

import scala.collection.mutable

/**
  * Created by chenjunzou on 2017/3/20.
  */
class ReliableNSQReceiver(
        nsqParams: Map[String, String],
        storageLevel: StorageLevel)
  extends AbstractNSQReceiver(nsqParams, storageLevel)
    with LazyLogging {

  private var _blockGenerator: BlockGenerator = null

  val messageHandler: MessageHandler = new BaseMessageHandler(this)

  def blockGenerator: BlockGenerator = _blockGenerator



  override def onStart(): Unit = {
    var newParam: Map[String, String] = nsqParams
    if (nsqParams("nsq.auto.ack").toBoolean) {
      logger.warn("nsq.auto.ack should be turn off in reliable mode")
    }
    super.onStart()
    _blockGenerator = supervisor.createBlockGenerator(new GeneratedBlockHandler)
    _blockGenerator.start()
  }

  /**
    * Store the ready-to-be-stored block and commit the related offsets to zookeeper. This method
    * will try a fixed number of times to push the block. If the push fails, the receiver is stopped.
    */
  private def storeBlockAndAck(
        blockId: StreamBlockId, arrayBuffer: mutable.ArrayBuffer[_]): Unit = {
    var count = 0
    var pushed = false
    var exception: Exception = null
    while (!pushed && count <= 3) {
      try {
        store(arrayBuffer.asInstanceOf[mutable.ArrayBuffer[NSQMessageWrapper]])
        pushed = true
      } catch {
        case ex: Exception =>
          count += 1
          exception = ex
      }
    }
    if (pushed) {
      logger.info("block " + blockId + " pushed " + arrayBuffer.length + "messages")
      for (msg <- arrayBuffer) {
        consumer.finish(msg.asInstanceOf[NSQMessageWrapper].getMessage)
      }
    } else {
      stop("Error while storing block into Spark", exception)
    }
  }

  /** Class to handle blocks generated by the block generator. */
  private final class GeneratedBlockHandler extends BlockGeneratorListener {

    def onAddData(data: Any, metadata: Any): Unit = {
    }

    def onGenerateBlock(blockId: StreamBlockId): Unit = {
    }

    def onPushBlock(blockId: StreamBlockId, arrayBuffer: mutable.ArrayBuffer[_]): Unit = {
      // Store block and commit the blocks offset
      storeBlockAndAck(blockId, arrayBuffer)
    }

    def onError(message: String, throwable: Throwable): Unit = {
      reportError(message, throwable)
    }
  }
}
